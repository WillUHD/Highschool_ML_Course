<img align="right" width="128" height="110" alt="SHSID Data Science Club" src="https://github.com/user-attachments/assets/8bab7372-fd36-4664-90d8-eee7113fd75a" />

<div align="left">

High School ML Course
=====================
created by the SHSID Data Science Club

---

A machine learning course designed for high school students, inspired by the USAAIO course outline provided by Beaver-Edge.

**For the best experience and visualization, download this repository and run the `ipynb`!**

---

# Table of Contents

### 0. Prerequisites
* 0.1. Basic Environment For Python
* **0.2. Python For AI**
    * 0.2.1. Advanced Python Techniques For AI
    * 0.2.2. NumPy
    * 0.2.3. Pandas
    * 0.2.4. Matplotlib
    * 0.2.5. Seaborn

### 1. Mathematical Methods For AI
* 1.1. Linear Algebra
* **1.2. Calculus**
    * 1.2.1. Single-Variable Derivatives
    * 1.2.2. Multi-Variable Derivatives & Gradients
    * 1.2.3. Chain Rule
* **1.3. Probability & Statistics**
    * 1.3.1. Discrete Distributions
    * 1.3.2. Continuous Distributions
    * 1.3.3. Mean
    * 1.3.4. Variance, Covariance
    * 1.3.5. Bayes' Rule
* **1.4. Convex Optimization**
    * 1.4.1. Convexity
    * 1.4.2. Gradient Descent
    * 1.4.3. Duality

### 2. Machine Learning Generics
* 2.0. Machine Learning Terminology
* 2.1. Linear Regression & Logistic Regression
* 2.1. Support Vector Machines
* 2.3. Regularization, Bias-Variance Trade-Off, Kernel Methods, Cross Validation
* 2.4. Principal Component Analysis, Dimensionality Reduction
* 2.5. Decision Trees, Random Forests
* 2.5. K-Nearest Neighbors, Clustering K-Means
* 2.7. Bagging & Boosting (XGBoost)

### 3. PyTorch
* 3.1. Tensors
* 3.2. Autograd
* 3.3. Devices
* 3.4. Modules
* 3.5. Datasets
* 3.6. Dataloader
* 3.7. Losses
* 3.8. Optimizers

### 4. Deep Learning & Computer Vision
* 4.1. Forward Propagation, Activation Functions, Linear Layer
* 4.2. Backpropagation, Gradient Descent, Adaptive Moment Estimation
* 4.3. Parameter Initialization, Batch Normalization, Dropout
* 4.4. Convolutional Layers, Pooling Layers, Convolutional Neural Network
* 4.5. Image Data Augmentation
* 4.6. VGG
* 4.7. ResNet
* 4.8. GoogLeNet
* 4.9. Transfer Learning
* 4.10. Recurring Neural Networks
* 4.11. Reinforcement Learning

### 5. Transformers
* **5.1. Attention**
    * 5.1.1. Self-Attention
    * 5.1.2. Cross-Attention, Masked Self-Attention, Multi-Head-attention
    * 5.1.3. Layer Normalization, Word Embedding, Positional Encoding
* 5.2. Inference
* 5.3. Training, Pre-Training, Fine-Tuning
* 5.4. Batch Processing
* 5.5. BERT, T5, GPT
* 5.6. RL with Human Feedback for LLMs
* 5.7. Modern LLM Optimizers, Mixture-of-Experts, Retrieval Augmented Generation (DeepSeek Case Study)

### 6. Natural Language Processing & Graph Neural Networks
* 6.1. Character, Subword & Word Tokenization
* 6.2. Word Embedding Methods
* 6.3. Skip-Gram, Continuous Word Bag, Global Vectors
* 6.4. Encoder-Only & Decoder-Only Transformers
* 6.5. Message-Passing Neural Networks
* 6.6. Graph Convolutional Networks
* 6.7. Vision Transformers

### 7. OpenCV & Generative AI
* 7.1. Object Detection
* 7.2. UNet
* 7.3. Autoencoder, Variational Autoencoder
* 7.4. Generative Adversarial Network, Adversarial Attack
* 7.5. Stable Diffusion, Denoising Diffusion Probabilistic Methods
* 7.6. State Space Models, Selective State Spaces (Mamba Case Study)


